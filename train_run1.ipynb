{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-29 15:34:34.232440: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-29 15:34:34.272260: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-29 15:34:35.516421: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.models import resnet34\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image  \n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "train_data = pd.read_csv(\"/home/woody/iwso/iwso092h/ucb_kaggle/data/train.csv\")\n",
    "image_folder = r'/home/woody/iwso/iwso092h/ucb_kaggle/train_thumbnails'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframe(dataframe, image_folder):\n",
    "    # Get a list of image files in the folder\n",
    "    image_files = os.listdir(image_folder)\n",
    "\n",
    "    # Create a set of image IDs from the image files\n",
    "    image_ids_in_folder = {int(filename.split('_')[0]) for filename in image_files}\n",
    "\n",
    "    # Filter the dataframe to keep only rows with image IDs present in the folder\n",
    "    dataframe_filtered = dataframe[dataframe['image_id'].isin(image_ids_in_folder)]\n",
    "    \n",
    "    # train_df, val_df = train_test_split(dataframe_filtered, test_size=0.2, stratify=dataframe_filtered.label)\n",
    "    \n",
    "    dataframe = pd.get_dummies(dataframe_filtered, columns=['label'])\n",
    "    # val_op = pd.get_dummies(val_df, columns=['label'])\n",
    "    \n",
    "    return dataframe.reset_index(drop=True)\n",
    "\n",
    "preprocess_data = preprocess_dataframe(train_data, image_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassificationDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, custom_dataset, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.custom_dataset = custom_dataset\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        num_data = len(self.custom_dataset)\n",
    "        train_size = int(0.8 * num_data)\n",
    "        val_size = num_data - train_size\n",
    "        self.train_data, self.val_data = random_split(self.custom_dataset, [train_size, val_size])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, batch_size=self.batch_size, shuffle=True, num_workers=47)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_data, batch_size=self.batch_size, num_workers=47)\n",
    "    \n",
    "class ImageClassificationModel(pl.LightningModule):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(ImageClassificationModel, self).__init__()\n",
    "        self.resnet34 = resnet34(pretrained=True)\n",
    "        self.resnet34.fc = nn.Linear(self.resnet34.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet34(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        return loss\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        # Log training loss at the end of each epoch\n",
    "        avg_loss = torch.stack(outputs).mean()\n",
    "        self.log('train_loss', avg_loss, on_step=False, on_epoch=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        return loss\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # Log validation loss at the end of each epoch\n",
    "        avg_loss = torch.stack(outputs).mean()\n",
    "        self.log('val_loss', avg_loss, on_step=False, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import Callback\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "class BalancedAccuracyCallback(Callback):\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        for batch in trainer.datamodule.val_dataloader():\n",
    "            inputs, labels = batch\n",
    "            outputs = pl_module(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)  # Assuming outputs are logits, get the predicted class index\n",
    "            val_preds.extend(predicted.cpu().tolist())\n",
    "            val_labels.extend(labels.cpu().argmax(dim=1).tolist())  # Convert one-hot encoded labels to class indices\n",
    "\n",
    "        balanced_acc = balanced_accuracy_score(val_labels, val_preds)\n",
    "        trainer.logger.experiment.add_scalar(\"val_balanced_acc\", balanced_acc, global_step=trainer.global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCancerDataset(Dataset):\n",
    "    def __init__(self, metadata_df, image_folder, transform=None):\n",
    "        self.metadata_df = metadata_df\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transforms.Compose(\n",
    "                [transforms.Resize((224, 224)),\n",
    "                 transforms.ToTensor(), \n",
    "                 transforms.Normalize(mean=[0.48828688, 0.42932517, 0.49162089], std=[0.41380908, 0.37492874, 0.41795654])]\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_ids = self.metadata_df.image_id[idx]  \n",
    "        image_name = os.path.join(self.image_folder, \"{}_thumbnail.png\".format(image_ids))\n",
    "        # print(image_name)\n",
    "        image = Image.open(image_name)\n",
    "        label_CC  = self.metadata_df.label_CC[idx].astype(int)  \n",
    "        label_EC  = self.metadata_df.label_EC[idx].astype(int)    \n",
    "        label_HGSC  = self.metadata_df.label_HGSC[idx].astype(int)    \n",
    "        label_LGSC  = self.metadata_df.label_LGSC[idx].astype(int)    \n",
    "        label_MC  = self.metadata_df.label_MC[idx].astype(int)    \n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor([label_CC, label_EC, label_HGSC, label_LGSC, label_MC], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImageClassificationModel(num_classes=5)\n",
    "\n",
    "# from datasets import CustomCancerDataset\n",
    "image_folder = r'/home/woody/iwso/iwso092h/ucb_kaggle/train_thumbnails'\n",
    "custom_dataset = CustomCancerDataset(metadata_df=preprocess_data, image_folder=image_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = ImageClassificationDataModule(custom_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger  # Import the TensorBoardLogger\n",
    "logger = TensorBoardLogger(\"logs\", name=\"image_classification_logs\")  # Specify the log directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=10, \n",
    "    accelerator = 'cpu',\n",
    "    logger=logger, \n",
    "    log_every_n_steps=1, \n",
    "    callbacks=[BalancedAccuracyCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Support for `training_epoch_end` has been removed in v2.0.0. `ImageClassificationModel` implements this method. You can use the `on_train_epoch_end` hook instead. To access outputs, save them in-memory as instance attributes. You can find migration examples in https://github.com/Lightning-AI/lightning/pull/16520.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/woody/iwso/iwso092h/ucb_kaggle/train_run1.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bwoody.nhr.fau.de/home/woody/iwso/iwso092h/ucb_kaggle/train_run1.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, data_module)\n",
      "File \u001b[0;32m/home/woody/iwso/iwso092h/miniconda/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:545\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING\n\u001b[1;32m    544\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 545\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    546\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    547\u001b[0m )\n",
      "File \u001b[0;32m/home/woody/iwso/iwso092h/miniconda/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/home/woody/iwso/iwso092h/miniconda/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:581\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    575\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    576\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    577\u001b[0m     ckpt_path,\n\u001b[1;32m    578\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    580\u001b[0m )\n\u001b[0;32m--> 581\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    583\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    584\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/home/woody/iwso/iwso092h/miniconda/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:938\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_connector\u001b[39m.\u001b[39m_attach_model_callbacks()\n\u001b[1;32m    936\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_connector\u001b[39m.\u001b[39m_attach_model_logging_functions()\n\u001b[0;32m--> 938\u001b[0m _verify_loop_configurations(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    940\u001b[0m \u001b[39m# hook\u001b[39;00m\n\u001b[1;32m    941\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: preparing data\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/woody/iwso/iwso092h/miniconda/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:37\u001b[0m, in \u001b[0;36m_verify_loop_configurations\u001b[0;34m(trainer)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnexpected: Trainer state fn must be set before validating loop configuration.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39m==\u001b[39m TrainerFn\u001b[39m.\u001b[39mFITTING:\n\u001b[0;32m---> 37\u001b[0m     __verify_train_val_loop_configuration(trainer, model)\n\u001b[1;32m     38\u001b[0m     __verify_manual_optimization_support(trainer, model)\n\u001b[1;32m     39\u001b[0m \u001b[39melif\u001b[39;00m trainer\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39m==\u001b[39m TrainerFn\u001b[39m.\u001b[39mVALIDATING:\n",
      "File \u001b[0;32m/home/woody/iwso/iwso092h/miniconda/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:81\u001b[0m, in \u001b[0;36m__verify_train_val_loop_configuration\u001b[0;34m(trainer, model)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39m# check legacy hooks are not present\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mgetattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39mtraining_epoch_end\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)):\n\u001b[0;32m---> 81\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m     82\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSupport for `training_epoch_end` has been removed in v2.0.0. `\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(model)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m` implements this\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m method. You can use the `on_train_epoch_end` hook instead. To access outputs, save them in-memory as\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     84\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m instance attributes.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m You can find migration examples in https://github.com/Lightning-AI/lightning/pull/16520.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     86\u001b[0m     )\n\u001b[1;32m     87\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mgetattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39mvalidation_epoch_end\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)):\n\u001b[1;32m     88\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m     89\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSupport for `validation_epoch_end` has been removed in v2.0.0. `\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(model)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m` implements this\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     90\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m method. You can use the `on_validation_epoch_end` hook instead. To access outputs, save them in-memory as\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     91\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m instance attributes.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m You can find migration examples in https://github.com/Lightning-AI/lightning/pull/16520.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m     )\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Support for `training_epoch_end` has been removed in v2.0.0. `ImageClassificationModel` implements this method. You can use the `on_train_epoch_end` hook instead. To access outputs, save them in-memory as instance attributes. You can find migration examples in https://github.com/Lightning-AI/lightning/pull/16520."
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
