{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = r'/home/woody/iwso/iwso092h/mad_ucb_kaggle'\n",
    "train_images = r'train_thumbnails'\n",
    "image_folder = r'/home/woody/iwso/iwso092h/mad_ucb_kaggle/train_thumbnails'\n",
    "train_df = pd.read_csv(os.path.join(root,'data','train.csv'))\n",
    "train_wo_tma = train_df[train_df.is_tma==False]\n",
    "train_enc = pd.get_dummies(train_wo_tma, columns=['label'])\n",
    "train_enc[['label_CC',\n",
    "       'label_EC', 'label_HGSC', 'label_LGSC', 'label_MC']] = train_enc[['label_CC',\n",
    "       'label_EC', 'label_HGSC', 'label_LGSC', 'label_MC']].astype(int)\n",
    "train_split, val_split = train_test_split(train_enc, test_size=0.2, random_state=17)\n",
    "train_split = train_split.reset_index(drop=True)\n",
    "val_split = val_split.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tiles(img, tile_size=256, n_tiles=30, mode=0):\n",
    "    h, w, c = img.shape\n",
    "    pad_h = (tile_size - h % tile_size) % tile_size + ((tile_size * mode) // 2)\n",
    "    pad_w = (tile_size - w % tile_size) % tile_size + ((tile_size * mode) // 2)\n",
    "\n",
    "    img = np.pad(\n",
    "        img,\n",
    "        [[pad_h // 2, pad_h - pad_h // 2], [pad_w // 2, pad_w - pad_w // 2], [0, 0]],\n",
    "        constant_values=0,\n",
    "    )\n",
    "    img = img.reshape(\n",
    "        img.shape[0] // tile_size, tile_size, img.shape[1] // tile_size, tile_size, 3\n",
    "    )\n",
    "    img = img.transpose(0, 2, 1, 3, 4).reshape(-1, tile_size, tile_size, 3)\n",
    "    \n",
    "    idxs = np.argsort(img.reshape(img.shape[0], -1).sum(-1))\n",
    "    if len(img) < n_tiles:\n",
    "        img = np.pad(\n",
    "            img, [[0, n_tiles - len(img)], [0,0], [0,0], [0,0]], constant_values=255\n",
    "        )\n",
    "    # idxs = np.argsort(-img.reshape(img.shape[0], -1).sum(-1))[:n_tiles]\n",
    "    # print(type(idxs))\n",
    "    if idxs.shape[0]>n_tiles:\n",
    "        idxs = idxs[-n_tiles:]\n",
    "    img = img[idxs]\n",
    "    \n",
    "    return img\n",
    "\n",
    "def concat_tiles(tiles, n_tiles, image_size):\n",
    "    idxes = list(range(n_tiles))\n",
    "    \n",
    "    n_row_tiles = int(np.sqrt(n_tiles))\n",
    "    img = np.zeros(\n",
    "        (image_size*n_row_tiles, image_size*n_row_tiles, 3), dtype=\"uint8\"\n",
    "    )\n",
    "    \n",
    "    for h in range(n_row_tiles):\n",
    "        for w in range(n_row_tiles):\n",
    "            i = h * n_row_tiles + w\n",
    "            if len(tiles) > idxes[i]:\n",
    "                this_img = tiles[idxes[i]]\n",
    "            else:\n",
    "                this_img = np.ones((image_size, image_size, 3), dtype=\"uint8\") * 255\n",
    "                \n",
    "            h1 = h * image_size\n",
    "            w1 = w * image_size\n",
    "            img[h1 : h1 + image_size, w1 : w1 + image_size] = this_img\n",
    "    return img\n",
    "\n",
    "def sort_tiles_by_intensity(tiles):\n",
    "    intensities = np.mean(tiles, axis=(1, 2, 3))  # Calculate mean intensity for each tile\n",
    "    \n",
    "    sorted_indices = np.argsort(-intensities)\n",
    "    \n",
    "    sorted_tiles = tiles[sorted_indices]\n",
    "    sorted_intensities = intensities[sorted_indices]\n",
    "    \n",
    "    return sorted_tiles, sorted_intensities, sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def to_tensor(x):\n",
    "    x = x.astype(\"float32\") / 255\n",
    "\n",
    "    return torch.from_numpy(x).permute(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCancerDataset(Dataset):\n",
    "    def __init__(self, metadata_df, image_folder, transform=None):\n",
    "        self.metadata_df = metadata_df\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform  # Use the provided transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_ids = self.metadata_df.image_id[idx]  \n",
    "        image_name = os.path.join(self.image_folder, \"{}_thumbnail.png\".format(image_ids))\n",
    "        image = Image.open(image_name)\n",
    "        img = get_tiles(\n",
    "            np.array(image),\n",
    "            mode=0, n_tiles= 64\n",
    "        )\n",
    "        \n",
    "        sorted_img = sort_tiles_by_intensity(img)\n",
    "        img = concat_tiles(\n",
    "            img, 64, 256\n",
    "        )\n",
    "\n",
    "        # img = to_tensor(img)\n",
    "        img = Image.fromarray(img)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        label_CC  = self.metadata_df.label_CC[idx].astype(int)  \n",
    "        label_EC  = self.metadata_df.label_EC[idx].astype(int)    \n",
    "        label_HGSC  = self.metadata_df.label_HGSC[idx].astype(int)    \n",
    "        label_LGSC  = self.metadata_df.label_LGSC[idx].astype(int)    \n",
    "        label_MC  = self.metadata_df.label_MC[idx].astype(int)\n",
    "\n",
    "        return img, torch.tensor([label_CC, label_EC, label_HGSC, label_LGSC, label_MC], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "    \n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_ucb_dataset = CustomCancerDataset(train_split, image_folder='/home/woody/iwso/iwso092h/mad_ucb_kaggle/train_thumbnails',\n",
    "                                        transform=data_transforms)\n",
    "val_ucb_dataset = CustomCancerDataset(val_split, image_folder='/home/woody/iwso/iwso092h/mad_ucb_kaggle/train_thumbnails',\n",
    "                                      transform=data_transforms)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_ucb_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ucb_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, loss, train_dataloader, valid_dataloader, device, batch_size, num_epoch, lr, lr_min, optim='sgd', init=True, scheduler_type='Cosine'):\n",
    "    def init_xavier(m): \n",
    "        #if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "    if init:\n",
    "        net.apply(init_xavier)\n",
    "\n",
    "    print('training on:', device)\n",
    "    net.to(device)\n",
    "    \n",
    "    if optim == 'sgd': \n",
    "        optimizer = torch.optim.SGD((param for param in net.parameters() if param.requires_grad), lr=lr,\n",
    "                                    weight_decay=0)\n",
    "    elif optim == 'adam':\n",
    "        optimizer = torch.optim.Adam((param for param in net.parameters() if param.requires_grad), lr=lr,\n",
    "                                     weight_decay=0)\n",
    "    elif optim == 'adamW':\n",
    "        optimizer = torch.optim.AdamW((param for param in net.parameters() if param.requires_grad), lr=lr,\n",
    "                                      weight_decay=0)\n",
    "    elif optim == 'ranger':\n",
    "        optimizer = Ranger((param for param in net.parameters() if param.requires_grad), lr=lr,\n",
    "                           weight_decay=0)\n",
    "    if scheduler_type == 'Cosine':\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=num_epoch, eta_min=lr_min)\n",
    "    \n",
    "    train_losses = []\n",
    "    train_acces = []\n",
    "    eval_acces = []\n",
    "    best_acc = 0.0\n",
    "    #Train\n",
    "    for epoch in range(num_epoch):\n",
    "\n",
    "        print(\"——————Start of training round {}——————\".format(epoch + 1))\n",
    "\n",
    "        \n",
    "        net.train()\n",
    "        train_acc = 0\n",
    "        for batch in tqdm(train_dataloader, desc='Train'):\n",
    "            imgs, targets = batch\n",
    "            imgs = imgs.to(device)\n",
    "            #targets = torch.cat(targets, dim = 0)\n",
    "            targets = targets.to(device)\n",
    "            output = net(imgs)\n",
    "\n",
    "            Loss = loss(output, targets)\n",
    "          \n",
    "            optimizer.zero_grad()\n",
    "            Loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, pred = output.max(1)\n",
    "            num_correct = (pred == targets).sum().item()\n",
    "            acc = num_correct / (batch_size)\n",
    "            train_acc += acc\n",
    "        scheduler.step()\n",
    "        print(\"epoch: {}, Loss: {}, Acc: {}\".format(epoch, Loss.item(), train_acc / len(train_dataloader)))\n",
    "        train_acces.append(train_acc / len(train_dataloader))\n",
    "        train_losses.append(Loss.item())\n",
    "\n",
    "        \n",
    "        net.eval()\n",
    "        eval_loss = 0\n",
    "        eval_acc = 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, targets in valid_dataloader:\n",
    "                imgs = imgs.to(device)\n",
    "                #targets = torch.cat(targets, dim = 0)\n",
    "                targets = targets.to(device)\n",
    "                output = net(imgs)\n",
    "                Loss = loss(output, targets)\n",
    "                _, pred = output.max(1)\n",
    "                num_correct = (pred == targets).sum().item()\n",
    "                eval_loss += Loss\n",
    "                acc = num_correct / imgs.shape[0]\n",
    "                eval_acc += acc\n",
    "\n",
    "            eval_losses = eval_loss / (len(valid_dataloader))\n",
    "            eval_acc = eval_acc / (len(valid_dataloader))\n",
    "            if eval_acc > best_acc:\n",
    "                best_acc = eval_acc\n",
    "                torch.save(net.state_dict(),'best_acc.pth')\n",
    "            eval_acces.append(eval_acc)\n",
    "            print(\"Loss on the overall validation set: {}\".format(eval_losses))\n",
    "            print(\"Correctness on the overall validation set: {}\".format(eval_acc))\n",
    "    return train_losses, train_acces, eval_acces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs=10, save_path='model'):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in tqdm(train_loader, desc='training:', leave=False):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.float())  # Convert labels to float\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            for inputs, labels in tqdm(val_loader, desc='validating', leave=False):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)  # Ensure that your loss function matches the task\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate predictions\n",
    "                _, preds = torch.max(outputs, 1)  # Get the predicted class\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # Calculate accuracy or other relevant metrics\n",
    "            # val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "                    # val_balanced_accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "              f\"Train Loss: {running_loss/len(train_loader):.4f} \"\n",
    "              f\"Val Loss: {val_loss/len(val_loader):.4f} \"\n",
    "            #   f\"Val Balanced Accuracy: {val_accuracy:.4f}\"\n",
    "                )\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "        if (epoch + 1) % 3 == 0:\n",
    "            torch.save(model.state_dict(), f'{save_path}epoch_{epoch+1}_mobile2.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-02 00:43:55.361113: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-02 00:43:59.092694: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-02 00:44:06.115562: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, ViTForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import timm\n",
    "\n",
    "class ViT(pl.LightningModule):\n",
    "    def __init__(self, num_classes, image_size=224, backbone='vit_base_patch16_224', finetune_layer=True):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(backbone, pretrained=True)\n",
    "        \n",
    "        # Freeze all layers except the last one for transfer learning\n",
    "        if finetune_layer:\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "            self.model.head = nn.Identity()\n",
    "            self.finetune_layer = nn.Linear(self.model.head.in_features, num_classes)\n",
    "        else:\n",
    "            self.model.head = nn.Linear(self.model.head.in_features, num_classes)\n",
    "            self.finetune_layer = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        \n",
    "        if self.finetune_layer is not None:\n",
    "            logits = self.finetune_layer(logits)\n",
    "\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "\n",
    "        if self.finetune_layer is not None:\n",
    "            logits = self.finetune_layer(logits)\n",
    "\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.finetune_layer is not None:\n",
    "            parameters = list(self.model.parameters()) + list(self.finetune_layer.parameters())\n",
    "        else:\n",
    "            parameters = self.parameters()\n",
    "\n",
    "        optimizer = torch.optim.Adam(parameters, lr=1e-4)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | VisionTransformer | 85.8 M\n",
      "--------------------------------------------\n",
      "85.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "85.8 M    Total params\n",
      "343.210   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a4fc82cdd8b40c39e1b5f518ab2006b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/woody/iwso/iwso092h/miniconda/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n",
      "/home/woody/iwso/iwso092h/miniconda/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:293: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6f48a8adfb4a609b4b0abb1a6765ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "756a4ad19e6b4c90b0e8ccb710e64cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f96fa6f0ba50492089871af4f4461520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b084df0dfb14e1ab3b8a3d26e31fd66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/woody/iwso/iwso092h/miniconda/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "model = ViT(num_classes=5, finetune_layer=True)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10)  # Adjust max_epochs and gpus as needed\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
